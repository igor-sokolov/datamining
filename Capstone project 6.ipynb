{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Hygiene Prediction #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basePath = 'dataminingcapstone-001'\n",
    "hygienePath = 'Hygiene'\n",
    "workingDir = os.path.join(os.curdir, basePath, hygienePath)\n",
    "\n",
    "reviewsPath = os.path.join(workingDir, 'hygiene.dat')\n",
    "labelsPath = os.path.join(workingDir, 'hygiene.dat.labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tStraightforward solution ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 546\n",
    "\n",
    "with open(reviewsPath, 'r') as f:\n",
    "    data_train = [next(f) for x in xrange(N)]\n",
    "    data_pred = [x for x in f]\n",
    "    \n",
    "with open(labelsPath, 'r') as f:\n",
    "    y_train = [next(f) for x in xrange(N)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 546\n",
      "Predicted data length: 12753\n"
     ]
    }
   ],
   "source": [
    "print \"Train data length: {}\".format(len(data_train))\n",
    "print \"Predicted data length: {}\".format(len(data_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_simple = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=2, stop_words='english')\n",
    "X_train = tfidf_simple.fit_transform(data_train)\n",
    "X_test = tfidf_simple.transform(data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedLabelsPath = os.path.join(workingDir, 'output1.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tPreprocessing improvements ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I'll make simple checks a couple cases for chosen preprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = u'I\\'ll be waiting for you! I don\\'t mind. What\\'s the hell? I\\'m gonna home!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = \"\"\"!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "\n",
    "remove_punctuation_map = dict((ord(char), None) for char in punctuation)\n",
    "s = s.translate(remove_punctuation_map).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u\"'ll\", u'be', u'waiting', u'for', u'you', u'i', u'do', u\"n't\", u'mind', u'what', u\"'s\", u'the', u'hell', u'i', u\"'m\", u'gon', u'na', u'home']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "path_to_jar = '/Users/igorsokolov/stanford-postagger-2015-04-20/stanford-postagger-3.5.2.jar'\n",
    "tokens = StanfordTokenizer(path_to_jar=path_to_jar).tokenize(s)\n",
    "\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u\"'ll\", u'be', u'wait', u'for', u'you', u'i', u'do', u\"n't\", u'mind', u'what', u\"'s\", u'the', u'hell', u'i', u\"'m\", u'gon', u'na', u'home']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "stems = [porterStemmer.stem(item) for item in tokens]\n",
    "\n",
    "print stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exercises above we'r ready to implement subroutines for each step of pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I could use string.punctionation constant but it contains symbol ' which is used widely in casual speak. \n",
    "# So defined my own constant.\n",
    "punctuation = \"\"\"!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "\n",
    "remove_punctuation_map = dict((ord(char), None) for char in punctuation)\n",
    "\n",
    "def remove_punctuation(line):\n",
    "    return line.translate(remove_punctuation_map).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "path_to_jar = '/Users/igorsokolov/stanford-postagger-2015-04-20/stanford-postagger-3.5.2.jar'\n",
    "tokenizer = StanfordTokenizer(path_to_jar=path_to_jar, options={\"americanize\": True})\n",
    "\n",
    "def tokenize(line):\n",
    "    return tokenizer.tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "def stemming(tokens):\n",
    "    return [porterStemmer.stem(item) for item in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "reductions_map = {'\\'m': 'am', 'n\\'t': 'not', '\\'ll': 'will', '\\'s': 'is', '\\'ve': 'have', '\\'d': 'would',\n",
    "                 '\\'re': 'are'}\n",
    "\n",
    "def replace_reductions(line):\n",
    "    return reduce(lambda x, y: x.replace(y, reductions_map[y]), reductions_map, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewsRDD = sc.textFile(reviewsPath, use_unicode=True)\n",
    "labelsRDD = sc.textFile(labelsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"the baguett and roll are excel and although i have not tri them yet i am excit about the dozenplu type of fill croissant on offer at ridicul low price chees with or without ham blueberri with or without cream chees chocol almond thi could be danger i have a bad bakeri habit but at least at q bakeri i wo not go broke while i get fat i have tri four differ banh mi and i will agre with matthew that the basic one are somewhat american mushroom wa more interest than chicken or bbq pork obvious that made thi a good place to get food for the inlaw that seem exot but not too scari note to self do not publish thi on facebook where the inlaw might see it q bakeri is locat on two divid street make it a slight pain to get to but it is worth a stop if you want afford varieti thi use to be the locat of my favorit bahn mi shop in seattl king baguett so when i saw that after one year as a pho restaur that a new bahn mi joint had taken up resid with dream of king baguett ' chicken bahn mi danc in my head i quickli rush inat first examin q bakeri is a lot more welcom than the old king baguett there are a varieti of bake good in the case and they have both bahn mi and regular deli sandwich avail i had both a pork and a teriyaki chicken bahn mi and i found that they did not quit hit the mark on the plu side q bakeri is not stingi with the meat and the meat is both flavor and juici the teriyaki chicken wa especi interest as i have never had a bahn mi like it befor and i quit enjoy it also q bakeri is merci light on the mayonnais on the down side both my sandwich lack much kick they tast like an american bahn mi with all the asian flavor tone way down the baguett itself wa also kind of disapoint be more chewi than crustyfor a brand new place howev i think q bakeri ha a lot of potenti and i look forward to tri them out again in the near futur yum i alway alway look forward to visit my famili in seattl wa so i can get all type of bake goodi from q is they are known for their banh mi is i usual order the banh mi cha lua the custom servic is get better english is obvious not their second languag but mani establish these day are not anyway love thi place we mostli go there for the vietnames sandwich i usual go for the veggi one they are full of flavor tofu and a great assort of veggi i do not feel like i have just had a sandwich of bread after eat there it actual feel like i have eaten my veggi my husband get the meat sandwich he say he like q is sandwich becaus chicken is not shred the sandwich is bigger and the store is great becaus it is better lit better arrang and tidier than somehav live in franc and eaten my way though manyapatisseri i have to add that q is pastri select and tast are pretti awesom i am talk straight up delici pain au chocolat and pain aux raisin they also have all sort of candi in jar and sesam treat they have one or 2 tabl w 4 or 6 chair for eat in they are open til 7pm also they take card which make it handi if we do not have cashon star off becaus it is been a littl tough to commun with the folk there sinc we do not speak vietnames would not be a problem except we have end up with the wrong food befor overal great littl place and i will total be bring my parent there when they come to visit thi weekend thank for the idea matthewp brought my mom there when she came for a visit and she love it natur the ` rent had never had a vietnames sandwich befor great way to introduc them to the magic of banh mi\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prerocessed_reviews_RDD = (reviewsRDD\n",
    "                             .map(lambda line: line.lower())\n",
    "                             .map(lambda line: line.replace('&#160;', ''))\n",
    "                             .map(lambda line: remove_punctuation(line))\n",
    "                             .map(lambda line: tokenize(line))\n",
    "                             .map(lambda tokens: stemming(tokens))\n",
    "                             .map(lambda tokens: \" \".join(tokens))\n",
    "                             .map(lambda line: replace_reductions(line))\n",
    "                         )\n",
    "\n",
    "prerocessed_reviews_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prerocessed_reviews = prerocessed_reviews_RDD.collect()\n",
    "                       \n",
    "prerocessed_data_train = prerocessed_reviews[:N]\n",
    "prerocessed_data_pred = prerocessed_reviews[N:]  \n",
    "                       \n",
    "print \"Train data length: {}\".format(len(prerocessed_data_train))\n",
    "print \"Predicted data length: {}\".format(len(prerocessed_data_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.8,\n",
    "                                 stop_words='english')\n",
    "prerocessed_X_train = vectorizer.fit_transform(prerocessed_data_train)\n",
    "prerocessed_X_test = vectorizer.transform(prerocessed_data_pred)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % prerocessed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, solver='lsqr', tol=0.01)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"lsqr\")\n",
    "clf.fit(prerocessed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_pred = clf.predict(prerocessed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedLabelsPath = os.path.join(workingDir, 'output7.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in preprocessed_pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.\tAttempts to apply other classifiers ##\n",
    "\n",
    "### Chi and naive bayes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "select_chi2 = prerocessed_X_train.shape[1]\n",
    "ch2 = SelectKBest(chi2, k=select_chi2)\n",
    "prerocessed_X_train = ch2.fit_transform(prerocessed_X_train, y_train)\n",
    "prerocessed_X_test = ch2.transform(prerocessed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(prerocessed_X_train, y_train)\n",
    "bayes_pred = clf.predict(prerocessed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedLabelsPath = os.path.join(workingDir, 'output6.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in bayes_pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive agressive classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "clf.fit(prerocessed_X_train, y_train)\n",
    "\n",
    "pred = clf.predict(prerocessed_X_test)\n",
    "\n",
    "predictedLabelsPath = os.path.join(workingDir, 'output8.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes based classifiers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(prerocessed_X_train, y_train)\n",
    "\n",
    "pred = clf.predict(prerocessed_X_test)\n",
    "\n",
    "predictedLabelsPath = os.path.join(workingDir, 'output9.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Union text features with additional information ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addsPath = os.path.join(workingDir, 'hygiene.dat.additional')\n",
    "\n",
    "addsRDD = sc.textFile(addsPath, use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Vietnamese', 'Sandwiches', 'Restaurants'], ['98118', 4, 4.0])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def parseLine(line):\n",
    "    t = line.split('\"')\n",
    "    categories = re.compile(\"'(\\w*)'\").findall(t[1])\n",
    "    numbers = t[2].split(',')[1:]\n",
    "    \n",
    "    zip_code = numbers[0]\n",
    "    review_count = int(numbers[1])\n",
    "    rating = float(numbers[2])\n",
    "    \n",
    "    return categories, [zip_code, review_count, rating] \n",
    "    \n",
    "parseLine(\"\\\"['Vietnamese', 'Sandwiches', 'Restaurants']\\\",98118,4,4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additionals = addsRDD.map(lambda x: parseLine(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict_vectorizer = DictVectorizer()\n",
    "\n",
    "dict_index_train = [{str(adds[0]): 1} for catetogories, adds in additionals[:N]]\n",
    "dict_index_test = [{str(adds[0]): 1} for catetogories, adds in additionals[N:]]\n",
    "\n",
    "index_X_train = dict_vectorizer.fit_transform(dict_index_train)\n",
    "index_X_test = dict_vectorizer.transform(dict_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "cat_dict_vectorizer = DictVectorizer()\n",
    "\n",
    "def map_items(categories_list):\n",
    "    return {cat: 1 for cat in categories_list}\n",
    "\n",
    "categories_map_train = [map_items(catetogories) for catetogories, adds in additionals[:N]]\n",
    "categories_map_test = [map_items(catetogories) for catetogories, adds in additionals[N:]]\n",
    "\n",
    "categories_map_X_train = cat_dict_vectorizer.fit_transform(categories_map_train)\n",
    "categories_map_X_test = cat_dict_vectorizer.transform(categories_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "ratings_train = [[float(x[1][1]), float(x[1][2])] for x in additionals[:N]]\n",
    "ratings_test = [[float(x[1][1]), float(x[1][2])] for x in additionals[N:]]\n",
    "\n",
    "print len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prerocessed_X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "combined_X_train = hstack([prerocessed_X_train, ratings_train, index_X_train, categories_map_X_train])\n",
    "combined_X_test = hstack([prerocessed_X_test, ratings_test, index_X_test, categories_map_X_test])\n",
    "\n",
    "print combined_X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, solver='lsqr', tol=0.01)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"lsqr\")\n",
    "clf.fit(combined_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_pred = clf.predict(combined_X_test)\n",
    "\n",
    "predictedLabelsPath = os.path.join(workingDir, 'output14.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in combined_pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(combined_X_train, y_train)\n",
    "svc_pred = svc.predict(combined_X_test)\n",
    "\n",
    "predictedLabelsPath = os.path.join(workingDir, 'output15.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in svc_pred:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tSelection of K best features and fine tuning of classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('feature_kbest', SelectKBest(k=20, score_func=<function chi2 at 0x108ac11b8>)), ('ridge_classifier', RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=True, solver='lsqr', tol=0.01))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "select_chi2 = combined_X_train.shape[1]\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "        ('feature_kbest', SelectKBest(chi2)), #f_classif\n",
    "        ('ridge_classifier', RidgeClassifier(tol=1e-2, solver=\"lsqr\"))\n",
    "        ])\n",
    "\n",
    "\n",
    "param_grid = dict(feature_kbest__score_func = [chi2, f_classif],\n",
    "                  feature_kbest__k=[10, 15, 20, 25, 30, 50, 100, 1000, 10000, 20000, 'all'],\n",
    "                  ridge_classifier__tol=[1e-8, 1e-6, 1e-4, 1e-2, 1e-1],\n",
    "                  ridge_classifier__solver=['auto', 'cholesky', 'lsqr', 'sparse_cg'],\n",
    "                  ridge_classifier__normalize=[True, False])\n",
    "\n",
    "grid_search = GridSearchCV(ridge_pipeline, param_grid=param_grid)\n",
    "grid_search.fit(combined_X_train, y_train)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_ridge_pipeline = Pipeline([\n",
    "        ('feature_kbest', SelectKBest(chi2, k=10)),\n",
    "        ('ridge_classifier', RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,\n",
    "                                             fit_intercept=True, max_iter=None, normalize=False, \n",
    "                                             solver='auto', tol=0.0001))\n",
    "        ])\n",
    "\n",
    "tuned_ridge_pipeline.fit(combined_X_train, y_train)\n",
    "tuned_ridge_pipeline_pred = tuned_ridge_pipeline.predict(combined_X_test)\n",
    "\n",
    "predictedLabelsPath = os.path.join(workingDir, 'output18.txt')\n",
    "with open(predictedLabelsPath, 'w') as f:\n",
    "    f.write('sis\\n')\n",
    "    for line in tuned_ridge_pipeline_pred:\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
